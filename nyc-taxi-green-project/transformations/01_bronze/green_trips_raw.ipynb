{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a96c23-f58e-422b-b9ac-e429fe0741cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Using Service Principal Instead of Storage credential and External Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa447e04-62f0-44a9-9857-0880d993a8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# storage_account = \"stnyctaxigreen\"\n",
    "\n",
    "# # Fetching from Azure Key Vault\n",
    "# client_id = dbutils.secrets.get(scope=\"kv-nyctaxi-scope\", key=\"sp-client-id\")\n",
    "# tenant_id = dbutils.secrets.get(scope=\"kv-nyctaxi-scope\", key=\"sp-tenant-id\")\n",
    "# client_secret = dbutils.secrets.get(scope=\"kv-nyctaxi-scope\", key=\"sp-client-secret\")\n",
    "\n",
    "# # Service Principle Authentication\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "# spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "# spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "# spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab7b701-4f90-4ba4-8432-dd3f97b1d475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LOADING FROM LANDING TO BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7def95c1-10d6-40a1-b095-268a458c04e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, col, expr, regexp_extract\n",
    "\n",
    "# path for module imports\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.append(root)\n",
    "\n",
    "# Force reload modules to ensure latest version\n",
    "if 'modules.transformations.metadata' in sys.modules:\n",
    "    importlib.reload(sys.modules['modules.transformations.metadata'])\n",
    "\n",
    "from modules.transformations import add_processed_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ed7bb8-2f85-4abe-a677-bdb50694a87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths and Configuration\n",
    "volume_path = \"/Volumes/nyctaxi/00_landing/data_sources/*/*\"\n",
    "landing_base = \"/Volumes/nyctaxi/00_landing/data_sources/\"\n",
    "checkpoint_path = \"abfss://bronze@stnyctaxigreen.dfs.core.windows.net/_checkpoints/green_taxi\"\n",
    "table_name = \"nyctaxi.01_bronze.green_trips_raw\"\n",
    "\n",
    "\n",
    "# Using Auto Loader with Volume path\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"pathGlobFilter\", \"*.parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \n",
    "    .load(volume_path))\n",
    "\n",
    "\n",
    "# Metadata columns\n",
    "df_time = add_processed_timestamp(df_stream)\n",
    "df_final = df_time.withColumnRenamed(\"processed_timestamp\", \"load_timestamp\") \\\n",
    "                    .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "\n",
    "# Writing data\n",
    "query = df_final.writeStream \\\n",
    "            .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "            .trigger(availableNow=True) \\\n",
    "            .toTable(table_name)\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "\n",
    "# --- LOGGING ----\n",
    "\n",
    "# We look for rows added in the last 5 minutes to identify the current batch\n",
    "last_batch_df = spark.read.table(table_name).filter(col(\"load_timestamp\") >= current_timestamp() - expr(\"INTERVAL 5 MINUTES\"))\n",
    "\n",
    "# Check if any rows were actually written\n",
    "if last_batch_df.limit(1).count() == 0:\n",
    "    print(\"No new data to load\")\n",
    "else:\n",
    "    # Extract unique months from the source_file path\n",
    "    loaded_months = last_batch_df.select(regexp_extract(col(\"source_file\"), r\"(\\d{4}-\\d{2})\", 1).alias(\"m\")) \\\n",
    "                                 .distinct().orderBy(\"m\").collect()\n",
    "    \n",
    "    months = [r['m'] for r in loaded_months]\n",
    "    \n",
    "    # Format the range string\n",
    "    date_range = f\"{months[0]}\" if len(months) == 1 else f\"{months[0]} to {months[-1]}\"\n",
    "    \n",
    "    print(f\"Successfully loaded: {date_range} to {table_name}\")\n",
    "\n",
    "    print(f\"Total records in {table_name}: {spark.read.table(table_name).count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7935245661711087,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "green_trips_raw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
