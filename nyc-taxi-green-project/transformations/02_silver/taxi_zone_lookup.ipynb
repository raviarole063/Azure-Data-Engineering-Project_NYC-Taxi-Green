{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e54887-3205-4423-b66e-44b0a4dec44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Using Service Principal Instead of Storage credential and External Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09555dff-ba4a-4a6c-acc1-91ca7ea4f10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# storage_account = \"stnyctaxigreen\"\n",
    "\n",
    "# # Fetching from Azure Key Vault\n",
    "# client_id = dbutils.secrets.get(scope=\"kv-nyctaxi-scope\", key=\"sp-client-id\")\n",
    "# tenant_id = dbutils.secrets.get(scope=\"kv-nyctaxi-scope\", key=\"sp-tenant-id\")\n",
    "# client_secret = dbutils.secrets.get(scope=\"kv-nyctaxi-scope\", key=\"sp-client-secret\")\n",
    "\n",
    "# # Service Principle Authentication\n",
    "# spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "# spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "# spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", client_id)\n",
    "# spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", client_secret)\n",
    "# spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4b622f-59e0-4e2e-b73d-63c3f4e389c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### LOADING FROM LANDING TO SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e1756d-5ba0-4cca-ad42-20b1d1812e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp, col, when\n",
    "from pyspark.sql.types import TimestampType, IntegerType\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8396ec97-345a-4d7e-bc13-413a30949730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths and Configuration\n",
    "volume_path = \"/Volumes/nyctaxi/00_landing/data_sources/taxi_zone_lookup/*.csv\"\n",
    "target_table = \"nyctaxi.02_silver.taxi_zone_lookup\"\n",
    "storage_path = \"abfss://silver@stnyctaxigreen.dfs.core.windows.net/taxi_zone_lookup\"\n",
    "\n",
    "# Read and Transform Source\n",
    "df_source = spark.read.format(\"csv\").option(\"header\", True).load(volume_path)\n",
    "\n",
    "df_source = df_source.select(\n",
    "    col(\"LocationID\").cast(IntegerType()).alias(\"location_id\"),\n",
    "    col(\"Borough\").alias(\"borough\"),\n",
    "    col(\"Zone\").alias(\"zone\"),\n",
    "    col(\"service_zone\"),\n",
    "    current_timestamp().alias(\"effective_date\"),\n",
    "    lit(None).cast(TimestampType()).alias(\"end_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98dd2d0-dda4-42d6-a23a-7c809e557c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize External Table if Missing\n",
    "if not spark.catalog.tableExists(target_table):\n",
    "    print(f\"Initial run: Creating EXTERNAL table {target_table} at {storage_path}\")\n",
    "    df_source.write.format(\"delta\").option(\"path\", storage_path).saveAsTable(target_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ce0147-9148-4b87-8c58-9a0a3437959e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### TEST DATA - DEMO PURPOSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f47713c-5681-45e0-8603-1ac945fdec1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insert new record to the source DataFrame\n",
    "df_new = spark.createDataFrame([(999, \"New Borough\", \"New Zone\", \"New Service Zone\")], schema=\"location_id int, borough string, zone string, service_zone string\") \\\n",
    "                .withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "df_source = df_new.union(df_source)\n",
    "\n",
    "\n",
    "# Updating record for location_id 1\n",
    "df_source = df_source.withColumn(\"borough\", when(col(\"location_id\")==1, \"NEWARK AIRPORT\").otherwise(col(\"borough\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0b6e95-7b9d-46d1-abdc-83cf7a71b5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### SCD TYPE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a995d09-c689-484b-9f31-f9cace734e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "end_timestamp = datetime.now()\n",
    "dt = DeltaTable.forName(spark, target_table)\n",
    "\n",
    "# CHECK 1: Close active rows whose attributes changed\n",
    "dt.alias(\"t\").merge(\n",
    "    source = df_source.alias(\"s\"),\n",
    "    condition = \"t.location_id = s.location_id AND t.end_date IS NULL AND (t.borough != s.borough OR t.zone != s.zone OR t.service_zone != s.service_zone)\"\n",
    ").whenMatchedUpdate(\n",
    "    set = { \"t.end_date\": lit(end_timestamp).cast(TimestampType()) }\n",
    ").execute()\n",
    "\n",
    "# CHECK 2: Insert new updated versions for just-closed IDs\n",
    "insert_id_list = [row.location_id for row in dt.toDF().filter(f\"end_date = '{end_timestamp}'\").select(\"location_id\").collect()]\n",
    "\n",
    "if len(insert_id_list) == 0:\n",
    "    print(\"No updated records to insert\")\n",
    "else:   \n",
    "    dt.alias(\"t\").merge(\n",
    "        source = df_source.alias(\"s\"),\n",
    "        condition = f\"s.location_id not in ({', '.join(map(str, insert_id_list))})\"\n",
    "    ).whenNotMatchedInsert(\n",
    "        values = {\n",
    "            \"t.location_id\": \"s.location_id\",\n",
    "            \"t.borough\": \"s.borough\",\n",
    "            \"t.zone\": \"s.zone\",\n",
    "            \"t.service_zone\": \"s.service_zone\",\n",
    "            \"t.effective_date\": current_timestamp(),\n",
    "            \"t.end_date\": lit(None).cast(TimestampType())\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "# CHECK 3: Insert brand-new keys (no historical row in target)\n",
    "dt.alias(\"t\").merge(\n",
    "    source = df_source.alias(\"s\"),\n",
    "    condition = \"t.location_id = s.location_id\"\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"t.location_id\": \"s.location_id\",\n",
    "        \"t.borough\": \"s.borough\",\n",
    "        \"t.zone\": \"s.zone\",\n",
    "        \"t.service_zone\": \"s.service_zone\",\n",
    "        \"t.effective_date\": current_timestamp(),\n",
    "        \"t.end_date\": lit(None).cast(TimestampType())\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(f\"Total records in {target_table}: {spark.read.table(target_table).count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "taxi_zone_lookup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
